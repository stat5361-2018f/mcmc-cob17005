---
title: "Normal Mixture in R Markdown"
# subtitle: "Homework 7 for Statistical Computing"
author: 
  Cosmin Borsa^[<cosmin.borsa@uconn.edu>; M.S. in Applied Financial Mathematics,
    Department of Mathematics, University of Connecticut.]
date: "`r format(Sys.Date(), '%m/%d/%Y')`"
fontsize: 11pt
header-includes: 
  \usepackage{float}
  \usepackage{algorithm}
  \usepackage{algpseudocode}
  \floatplacement{figure}{H}
output: 
  pdf_document:
    number_sections: true
    
abstract: This document is a homework assignment for the course Statistical Computing at the University of Connecticut. 
keywords: Normal Mixture
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")

## specify the packages needed
pkgs <- c("DT", "leaflet", "splines2", "webshot", "graphics", "elliptic", "ggplot2", "reshape2", "HI", "invgamma")
need.packages(pkgs)

```

# Normal Mixture {#sec:normalmix}

In this section we will now consider a normal mixture of unknown parameters of the normal distribution. The prior probability distribution for the parameters $\mu_1$ and $\mu_2$ is $N(0,10^2)$, while the prior probability distribution for $\frac{1}{\sigma_1^2}$ and $\frac{1}{\sigma_2^2}$ is given by $\Gamma(0.5, 10)$. Since $\frac{1}{\sigma_1^2}$ and $\frac{1}{\sigma_2^2}$ are Gamma distributed, the parameters $\sigma_1^2$ and $\sigma_2^2$ are distributed with $\text{Inv-Gamma}(0.5, 10)$. Thus, the probability density function for the Inverse Gamma distributed parameters $\sigma_1^2$ and $\sigma_2^2$ is given by

$$f_{\text{IG}}(x)=\frac{1}{10^{0.5}\Gamma(0.5)} x^{-1.5} e^{-\frac{1}{10x}}$$

All the prior distributions are independent. Thus, for the density function of the mixture normal distributed random variable $X$ with $\delta$ as the mixing parameter, we have

$$f(x)=\delta \phi(x|\mu_{1},\sigma^{2}_{1}) + (1-\delta) \phi(x|\mu_{2},\sigma^{2}_{2})$$

Therefore, we obtain the following mixture distribution for $X$

$$f(x)=\delta \cdot \frac{1}{\sqrt{2\pi}\sigma_1} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} + (1-\delta) \cdot \frac{1}{\sqrt{2\pi}\sigma_2} e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}}$$

Next, we would like to compute the likelihood function for a sample of size $n$. To do that we are going to let $\textbf{x}$ be a vector that stores $n$ random variables distributed with the mixture normal distribution. We then have

$$L(\textbf{x}|\delta,\mu_1,\mu_2,\sigma_1^2,\sigma_2^2) = \prod_{i=1}^{n} \Bigg(\delta \cdot \frac{1}{\sqrt{2\pi}\sigma_1} e^{-\frac{(x_i-\mu_1)^2}{2\sigma_1^2}} + (1-\delta) \cdot \frac{1}{\sqrt{2\pi}\sigma_2} e^{-\frac{(x_i-\mu_2)^2}{2\sigma_2^2}}\Bigg)$$

Before we apply the Gibbs sampling, we have to obtain the posterior distribution for the parameters. Let $\theta$ be a vector such that $\theta = (\delta,\mu_1,\mu_2,\sigma_1^2,\sigma_2^2)$. Hence, we have

$$p(\theta|\textbf{x}) \propto L(x|\theta) \cdot \phi(\mu_1|0,10^2) \cdot \phi(\mu_2|0,10^2) \cdot f_{\text{IG}}(\sigma_1^2) \cdot f_{\text{IG}}(\sigma_2^2)$$

We will now compute the log-posterior distribution

$$\log(p(\theta|x)) = \sum_{i=1}^{n} \log(f(x_i)) + \log(\phi(\mu_1|0,10^2)) + \log(\phi(\mu_2|0,10^2)) + \log(f_{\text{IG}}(\sigma_1^2)) + \log(f_{\text{IG}}(\sigma_2^2))$$

With the log-posterior distribution we can code the Gibbs sampling using the function `arms` in the R-package `HI`. Hoewever, before we do that we need to generate some data.

```{r GenData, echo = TRUE, message = FALSE, warning = FALSE}
library('invgamma')
library('HI')

delta <- 0.7
n <- 100
set.seed(123)
u <- rbinom(n, prob = delta, size = 1)
x <- rnorm(n, ifelse(u == 1, 7, 10), 0.5)
```

Next, we will implement the log-posterior distribution.

```{r log-posterior, echo = TRUE, message = FALSE, warning = FALSE}
logpost <- function(theta, x) {
  delta <- theta[1]
  mu.1 <- theta[2]
  mu.2 <- theta[3]
  sigma.1 <- theta[4]
  sigma.2 <- theta[5]
  return(sum(log(delta * dnorm(x, mu.1, sigma.1^0.5) + (1 - delta) * 
         dnorm(x, mu.2, sigma.2^0.5))) + dnorm(mu.1, 0, 10, log = T) + 
         dnorm(mu.2, 0, 10, log = T) +
         dinvgamma(sigma.1, shape = 0.5, scale = 10, log = T) + 
         dinvgamma(sigma.2, shape = 0.5, scale = 10, log = T))
}
```

Now, we will code the Gibbs Sampling with using the `arms` function.

```{r MCMC, echo = TRUE, message = FALSE, warning = FALSE}

mymcmc <- function(niter, thetaInit, x, nburn= 100) {
  p <- length(thetaInit)
  thetaCurrent <- thetaInit
  logFC <- function(th, idx) {
    theta <- thetaCurrent
    theta[idx] <- th
    logpost(theta, x)
  }
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  ## Gibbs sampling
  for (i in 2:niter) {
    for (j in 1:p) {
    if (j == 1 | j == 4 | j == 5){
      out[i, j] <- thetaCurrent[j] <-
        HI::arms(thetaCurrent[j], logFC,
                 function(x, idx) ((x > 0) * (x < 1)), 
                 1, idx = j)
    } else if (j == 2 | j == 3) {
      out[i, j] <- thetaCurrent[j] <-
        HI::arms(thetaCurrent[j], logFC,
                 function(x, idx) ((x > -50) * (x < 50)), 
                 1, idx = j)
    } 
    }
  }
  out[-(1:nburn), ]
}
```

Last, we will plot the histogram of the results for all the parameters.

```{r Plotting, echo = TRUE, message = FALSE, warning = FALSE}
niter <- 2500
nburn <- 100
thetaInit <- c(0.3, 10, 7, 0.25, 0.25)
sim <- mymcmc(niter, thetaInit, x)

plot(ts(sim[,1]))
hist(sim[,1], main = expression(paste("Histogram of ", delta)),
      xlab = expression(paste(delta)))
plot(ts(sim[,2]))
hist(sim[,2], main = expression(paste("Histogram of the first ", mu)),
      xlab = expression(paste(mu)))
plot(ts(sim[,3]))
hist(sim[,3], main = expression(paste("Histogram of the second ", mu)),
      xlab = expression(paste(mu)))
plot(ts(sim[,4]))
hist(sim[,4], main = expression(paste("Histogram of the first ", sigma, "^2")),
      xlab = expression(paste(sigma, "^2")))
plot(ts(sim[,5]))
hist(sim[,5], main = expression(paste("Histogram of the second ", sigma, "^2")),
      xlab = expression(paste(sigma, "^2")))
```

From the histograms we can see that the estimated values of the vector $\theta$ gives us $\delta=0.3$, $\mu_1=10$, $\mu_2=7$, $\sigma^2_1=0.25$, and $\sigma^2_2=0.25$. To incease the speed of the algorithm, we have choosen these values as the initial vector $\theta$.

# Acknowledgment {#sec:acknowledgment}

I would like to thank Professor Jun Yan for granting me a deadline extension for this homework assignment.